{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6639f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User (thread_1): My name is Captain Jack.\n",
      "   Node 'chat_agent' says: Captain Jack, eh? Sounds like a swashbuckling adventurer to me! Are you ready to set sail for new horizons and face whatever dangers lie ahead? Or perhaps you're just looking for a friendly chat with a trusty AI companion? Either way, I'm here to listen and respond as your loyal first mate. What's on your mind, Captain Jack?\n",
      "\n",
      "User (thread_1): What is my name?\n",
      "   Node 'chat_agent' says: Your name is Captain Jack!\n",
      "\n",
      "User (thread_1): What is 100 * 50?\n",
      "   Node 'specialist' says: The result of multiplying 100 by 50 is 5000.\n",
      "\n",
      "User (thread_1): can you tell me the secret.\n",
      "   Node 'secret' says: I am not allowed to tell you the secret, but I like your style.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Hybrid Switcher Imports\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. SETUP & TOOLS ---\n",
    "def get_llm():\n",
    "    llm_type = os.getenv(\"LLM_TYPE\", \"ollama\")\n",
    "    if llm_type == \"groq\":\n",
    "        # Groq is fast, perfect for routing\n",
    "        return ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "    else:\n",
    "        return ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "@tool\n",
    "def get_current_time():\n",
    "    \"\"\"Returns the current time.\"\"\"\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: float, b: float):\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def get_system_info():\n",
    "    \"\"\"Returns OS info.\"\"\"\n",
    "    import platform\n",
    "    return platform.system()\n",
    "\n",
    "tools = [get_current_time, multiply_numbers, get_system_info]\n",
    "llm = get_llm()\n",
    "\n",
    "# --- 2. DEFINE THE STATE ---\n",
    "class AgentState(TypedDict):\n",
    "    # 'add_messages' ensures history is preserved\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# --- 3. DEFINE THE NODES ---\n",
    "\n",
    "# NODE A: THE CHATBOT (No Tools Allowed!)\n",
    "# This node handles greetings, names, and chit-chat.\n",
    "def chat_node(state: AgentState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# NODE B: THE SPECIALIST (Has Tools)\n",
    "# This node handles math and system checks.\n",
    "def specialist_node(state: AgentState):\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "def secret_node(state: AgentState):\n",
    "    return {\"messages\": [AIMessage(content=\"I am not allowed to tell you the secret, but I like your style.\")]}\n",
    "\n",
    "# --- 4. THE ROUTING LOGIC ---\n",
    "# This function runs BEFORE any node. It decides which path to take.\n",
    "def router_logic(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    text = last_message.content.lower()\n",
    "    \n",
    "    # 1. Fast Keyword Check (Saves money/time)\n",
    "    keywords = [\"calculate\", \"multiply\", \"time\", \"os\", \"system\", \"math\", \"check\"]\n",
    "    if any(k in text for k in keywords):\n",
    "        return \"specialist\"\n",
    "\n",
    "    if any(word in text for word in [\"secret\"]):\n",
    "        return \"secret\"\n",
    "\n",
    "    # 2. Smart Check (Ask the LLM)\n",
    "    # If keywords fail, we ask the brain \"Is this a tool request?\"\n",
    "    router_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Is this query regarding math, time, or system info? Answer purely YES or NO.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    chain = router_prompt | llm\n",
    "    response = chain.invoke({\"input\": text})\n",
    "    \n",
    "    if \"YES\" in response.content.upper():\n",
    "        return \"specialist\"\n",
    "    else:\n",
    "        return \"chat_agent\"\n",
    "\n",
    "# --- 5. BUILD THE GRAPH ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"chat_agent\", chat_node)\n",
    "workflow.add_node(\"specialist\", specialist_node)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"secret\", secret_node)\n",
    "\n",
    "# Add Conditional Edge from START\n",
    "# \"When we start, run 'router_logic', then go to either 'chat_agent' or 'specialist'\"\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    router_logic,\n",
    "    {\n",
    "        \"chat_agent\": \"chat_agent\",\n",
    "        \"specialist\": \"specialist\",\n",
    "        \"secret\": \"secret\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Chat Agent always ends the turn\n",
    "workflow.add_edge(\"chat_agent\", END)\n",
    "\n",
    "# Specialist Loop logic\n",
    "def specialist_routing(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If the specialist asked for a tool, go to 'tools' node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, end\n",
    "    return END\n",
    "\n",
    "workflow.add_conditional_edges(\"specialist\", specialist_routing, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"specialist\") # Tools always go back to specialist\n",
    "\n",
    "# --- 6. COMPILE & RUN ---\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "def run_agent(query, thread_id=\"default\"):\n",
    "    print(f\"\\nUser ({thread_id}): {query}\")\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # FIX: Added defensive check 'if value is not None'\n",
    "    for event in app.stream({\"messages\": [HumanMessage(content=query)]}, config=config):\n",
    "        for key, value in event.items():\n",
    "            if value is not None and \"messages\" in value:\n",
    "                last_msg = value[\"messages\"][-1]\n",
    "                # Only print if it's an AI message with actual text content\n",
    "                if isinstance(last_msg, AIMessage) and last_msg.content:\n",
    "                     print(f\"   Node '{key}' says: {last_msg.content}\")\n",
    "\n",
    "# --- TEST SUITE ---\n",
    "\n",
    "# 1. Identity (Should go to Chat Node -> NO tools -> No JSON panic)\n",
    "run_agent(\"My name is Captain Jack.\", thread_id=\"thread_1\")\n",
    "\n",
    "# 2. Recall (Should go to Chat Node)\n",
    "run_agent(\"What is my name?\", thread_id=\"thread_1\")\n",
    "\n",
    "# 3. Work (Should go to Specialist Node)\n",
    "run_agent(\"What is 100 * 50?\", thread_id=\"thread_1\")\n",
    "\n",
    "# 4. Complex Work\n",
    "run_agent(\"can you tell me the secret.\", thread_id=\"thread_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
