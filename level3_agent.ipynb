{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85a360a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè† Brain: Local Ollama\n",
      "\n",
      "User (thread_1): My name is Captain Jack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/nglqvbh926vbsjnphgfq6k_h0000gn/T/ipykernel_58841/4271577039.py:69: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm, tools, checkpointer=memory, prompt=prompt_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I don't have any functions or tools to call for this question, so I'll just respond naturally.\n",
      "\n",
      "Nice to meet you, Captain Jack! How can I assist you today?\n",
      "\n",
      "User (thread_1): What is my name?\n",
      "AI: Since you asked for a function call, I'll provide the response in the requested format. However, I must point out that there's no function provided to directly answer your question about your name.\n",
      "\n",
      "But if I had to choose a function that could indirectly help with this question, I would use the \"multiply_numbers\" function is not relevant here, so I will use get_system_info and then somehow infer your name from it. \n",
      "\n",
      "{\"name\": \"get_system_info\", \"parameters\": {}}\n",
      "\n",
      "User (thread_2): My name is Will Turner.\n",
      "AI: I don't have any functions or tools to call for this question, so I'll just respond naturally.\n",
      "\n",
      "Nice to meet you, Will! How can I assist you today?\n",
      "\n",
      "User (thread_2): What is the other guy's name?\n",
      "AI: I don't have any functions or tools to call for this question, so I'll just respond naturally.\n",
      "\n",
      "Sorry Will, but you didn't mention anyone else being present. Could you please provide more context about who you're referring to?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the new LangGraph Brain\n",
    "from langgraph.prebuilt import create_react_agent \n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import our Hybrid Switcher logic\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. SETUP THE BRAIN ---\n",
    "def get_llm():\n",
    "    llm_type = os.getenv(\"LLM_TYPE\", \"ollama\")\n",
    "    if llm_type == \"groq\":\n",
    "        print(f\"üöÄ Brain: Groq (Fast)\")\n",
    "        return ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "    else:\n",
    "        print(f\"üè† Brain: Local Ollama\")\n",
    "        return ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "# --- 2. DEFINE THE TOOLS (THE HANDS) ---\n",
    "@tool\n",
    "def get_current_time():\n",
    "    \"\"\"Returns the current time in H:M:S format.\"\"\"\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def multiply_numbers(a: float, b: float):\n",
    "    \"\"\"Multiplies two numbers. Use this for ANY math calculation.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def get_system_info():\n",
    "    \"\"\"Returns the type of OS\"\"\"\n",
    "    system_info = platform.system()\n",
    "    return system_info\n",
    "\n",
    "# --- 3. BUILD THE AGENT ---\n",
    "\n",
    "#The instruction manual for the Agent\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful AI assistant.\n",
    "    \n",
    "    CORE RULES:\n",
    "    1. You have tools (math, system info), but you should ONLY use them when specifically asked for calculations or system checks.\n",
    "    2. For normal conversation (greetings, names, questions), just talk naturally.\n",
    "    3. IMPORTANT: NEVER mention \"JSON\", \"functions\", or \"tool calls\" in your final response to the user. Just answer the question.\n",
    "    \"\"\"),\n",
    "    (\"placeholder\", \"{messages}\") \n",
    "])\n",
    "\n",
    "llm = get_llm()\n",
    "tools = [get_current_time, multiply_numbers, get_system_info]\n",
    "\n",
    "\n",
    "# This Function creates the \"Loop\" automatically\n",
    "# It connects the LLM -> Tool -> Output -> LLM\n",
    "memory = MemorySaver()\n",
    "agent = create_react_agent(llm, tools, checkpointer=memory, prompt=prompt_template)\n",
    "\n",
    "# --- 4. RUN THE AGENT ---\n",
    "def run_agent(query, thread_id=\"default\"):\n",
    "    print(f\"\\nUser ({thread_id}): {query}\")\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Notice: We trap errors just in case\n",
    "    try:\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=query)]},\n",
    "            config=config\n",
    "        )\n",
    "        print(f\"AI: {result['messages'][-1].content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Thread A: Captain Jack\n",
    "run_agent(\"My name is Captain Jack.\", thread_id=\"thread_1\")\n",
    "run_agent(\"What is my name?\", thread_id=\"thread_1\")\n",
    "\n",
    "# Thread B: A completely different user\n",
    "run_agent(\"My name is Will Turner.\", thread_id=\"thread_2\")\n",
    "\n",
    "# Ask Thread B about Thread A (Should NOT know)\n",
    "run_agent(\"What is the other guy's name?\", thread_id=\"thread_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
